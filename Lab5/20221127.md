# AI工程程式與實作 - 自動訪問維基百科 + 資料工程與分析

<a name="000"/>

---
## 目錄
##### [5-1 起手式: 訪問 Wikipedia文章！](#001)
##### [5-2 下載中文維基百科, 一起學習文章語料分詞和詞頻分析!](#002)
##### [5-3 Your Show Time: 找一篇簡體文章或新聞 實作H1~H6](#003)
---

<a name="001"/>

### [5-1 起手式: 訪問 Wikipedia文章！]
![螢幕擷取畫面 2022-11-27 152108](https://user-images.githubusercontent.com/89327055/204123937-83f042ae-13a1-483f-b1f2-8e1485731c23.png)
````python
# Wikipedia: 元智大學"
# 5101 摘要
wikipedia.set_lang("zh-tw")
wikipedia.summary("元智大學", auto_suggest=False)

# 5101a
keyword = "元智大學"
wikipedia.page(keyword, auto_suggest=False).url

# 5103 Wiki 文章中的部分
wikipedia.page(keyword, auto_suggest=False).section('工學院時期')

# 5105 Wiki 文章中的部分
wikipedia.page(keyword, auto_suggest=False).section('大學時期')

# 5107 類別查詢

wikipedia.page(keyword, auto_suggest=False).categories

# 5109 在Wiki中直接Search
print(wikipedia.search(keyword))
print('找到了幾個?',len(wikipedia.search(keyword)))

# Show 5 個
print(wikipedia.search("元智大學", results = 5, suggestion = True))
````
[TOP](#000)

<a name="002"/>

### [5-2 下載中文維基百科, 一起學習文章語料分詞和詞頻分析!]
![螢幕擷取畫面 2022-11-27 154447](https://user-images.githubusercontent.com/89327055/204124586-e0b03f78-48ea-4ea4-a882-28104a4b8d05.png)
````python

#5101 zhwiki-中文維基 latest-最新的版本 xml的壓縮檔 (約10分鐘)
!wget https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2
# 1.78G  5.00MB/s    in 6m 9s

# 5102 Check the current path and folder content
!pwd
!ls -lrt


###5103 安裝與升級 gensim
!pip install -U gensim # -U --upgrade 模組升級
from gensim.corpora import WikiCorpus, wikicorpus

#5106 進行中文維基百科的語料清洗
%%time

wiki = 'zhwiki-latest-pages-articles.xml.bz2' #下載1.8GB的檔案

texts = 0 #文章計數器 將一篇文章(百科) 清洗成一行語料

wiki_corpus = WikiCorpus(wiki, dictionary={}) #讀入語料

#建立輸出檔案 參數( 檔名, 'w'覆蓋寫入 'a'寫入加在檔案尾端  )
with open("wiki_texts_001.txt",'w') as output: 
    for text in wiki_corpus.get_texts(): #依序將文字語料取出
        output.write(' '.join(text) + '\n') #使用空格分隔 寫入輸出檔案
        texts += 1 #文章計數器+1
        if texts % 10 == 0: #顯示進度 可修改
            print("已處理 %d 篇文章" % texts)
        if texts > 100: #可以自行修改需要的文章數 #mark掉 最後兩行就是全部輸出 處理時間長
            break


#5107 顯示擷取出來的語料 一行是一篇百科
!du -m wiki_texts_001.txt


#5108 head -10 顯示前10行 可修改 + 檔名
!head -10 wiki_texts_001.txt
#文章中符號已被洗掉 但還是有英文 簡體字

#5109 使用取代函數洗掉英文字 
import re #正則表達式函數
txt_in = open("wiki_texts_001.txt") #讀入檔案
for line in txt_in.readlines(): #從所有行readlines中 依序取出一行
    with open("out01.txt","a") as out: #寫入 檔名 "a" 參數新增一行 而非覆蓋
    #使用取代函數 re.sub( '英文字母' , ''空字串, 一行文章 )
        line = re.sub('[a-zA-Z]','',line) #0-9
        out.write(line) #寫入檔案

#5110 檢查英文是否洗掉
!head -10 out01.txt

"""
s2twp: 簡體中文 -> 繁體中文 (台灣, 包含慣用詞轉換)
tw2sp: 繁體中文 (台灣) -> 簡體中文 (包含慣用詞轉換)

t2tw: 繁體中文 -> 繁體中文 (台灣)
tw2s: 繁體中文 (台灣) -> 簡體中文
t2s: 繁體中文 -> 簡體中文

"""
#5111  使用轉換中文簡繁的工具 opencc
!pip install opencc-python-reimplemented

#5112 opencc 範例
from opencc import OpenCC
cc1 = OpenCC('s2tw') #直轉
cc2 = OpenCC('s2twp') #慣用語
text = '软件工程师'
print('直轉  ',cc1.convert(text))
print('慣用語',cc2.convert(text))

#5113 檔案簡體 轉 繁體慣用語
%%time
sc = OpenCC('s2twp')
txt_in = open("out01.txt") #讀入檔案 
count = 0
for line in txt_in.readlines(): #從所有行readlines中 依序取出一行 (e.g., 现代汉语词汇)
    with open("out_TW.txt","a") as out: #寫入 檔名 "a" 參數新增一行 而非覆蓋
        line2 = sc.convert(line) #簡體 轉 繁體慣用語
        out.write(line2) #寫入檔案
        if count < 3:        
          print(' 簡:',line, '繁:', line2)
          count = count + 1

#5114 檢查是否全部為繁體
!head -10 out_TW.txt

#5115 顯示檔案最後幾行
!tail -15 out_TW.txt

#5116 查詢檔案總共幾行
!cat out_TW.txt | wc -l


# 語料清洗 : 使用模組工具將不要的符號濾掉，保留我們要的文字語料
# 中文字: 雖然是最小單位，但同義字太多 中文詞: 是最小有意義，且可以自由使用的語言單位 任何語言的處理，都必須先能分辨文本中的詞，才能進行進下一步的處理 英文使用空格分隔詞，但中文卻沒有，因此中文需要分詞(or 斷詞)工具

#5117 jieba分詞工具 系統已安裝
!pip install jieba

#5118 分詞工具範例
import jieba
text1 = '新竹的交通大學在新竹的大學路上'
seg_list = jieba.cut(text1)
print(" , ".join(seg_list))

#5119 安裝繁體版jieba
!pip install git+https://github.com/APCLab/jieba-tw.git

#5120 繁體版分詞
import jieba
text1 = '新竹的交通大學在新竹的大學路上'
seg_list = jieba.cut(text1)
print(" , ".join(seg_list))
#要使用新版本 必須要Restart Runtime
#繁體版 大學路 成為一個詞

#### 有error? 找不到檔案??
!pwd

%cd "/content/drive/MyDrive/PEA2022/Lab5_Auto-WordFreq"

!ls -lrt

#5121 文章分詞

#jieba.set_dictionary('dict.txt.big') #使用繁體字典
output = open('out_TW_seg.txt', 'w')
with open('out_TW.txt', 'r') as txt_in :
    for texts_num, line in enumerate(txt_in): 
        line = line.strip('\n') #刪除Enter
        words = jieba.cut(line) #分詞
        for word in words:
            output.write(word + ' ') #空白分隔 每個詞
        output.write('\n') #文章結束跳行
        if (texts_num + 1) % 100 == 0:
            print("已完成前 %d 行的斷詞" % (texts_num + 1))
output.close()

#5124 分詞後結果
!tail -15 out_TW_seg.txt

#5128 取一行(文章)
fname="out_TW_seg.txt"
with open(fname, 'r', encoding='utf-8') as f:  # 打開文件
    lines = f.readlines()  # 讀取所有行
print(lines[0])

#5129 快速文章分析-詞頻
import jieba
import re
from collections import Counter #計數器模組
cut_words=""

all_words = lines[0].split(' ') #文章使用空格 全部加入list
print(all_words)

c=Counter() #建立計數器

for x in all_words:
    if len(x)>1 and x != '\r\n': #詞長度要大於1 不等於\r\n跳行符號
        c[x] += 1 #計數器+1

print('\n詞频統計：')
for (k,v) in c.most_common(20):# 詞频最高的前幾個詞
    print("%5s :%3d"%(k,v)) # 字串 : 數字


##5129b 最後用直方圖 + 折線圖來分析與表示 (H6)

import matplotlib.pyplot as plt
import numpy as np

names = []
values = []

for (k,v) in c.most_common(10):# 詞频最高的前10個詞, developed by Horace
  
    print("%5s :%3d"%(k,v)) # 字串 : 數字
    names.append(k)
    values.append(v)

fig, axs = plt.subplots(1, 2, figsize=(18, 6), sharey=True)
axs[0].bar(names, values)
axs[1].plot(names, values)
fig.suptitle('Categorical Plotting')

##########################################
# 如果中文的亂碼, 該怎麼辨?
# Colab 進行matplotlib繪圖時顯示繁體中文
# 下載台北思源黑體並命名taipei_sans_tc_beta.ttf，移至指定路徑
##########################################

!wget -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download

import matplotlib as mpl
import matplotlib.pyplot as plt 
from matplotlib.font_manager import fontManager

# 改style要在改font之前
# plt.style.use('seaborn')  

fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')
mpl.rc('font', family='Taipei Sans TC Beta')
##########################################

# fontManager.addfont('TaipeiSansTCBeta-Regular.ttf')
!ls -lrt

# #5129c 最後用直方圖 + 折線圖來分析與表示 (H6)

import matplotlib.pyplot as plt
import numpy as np

names = []
values = []

for (k,v) in c.most_common(10):# 詞频最高的前10個詞, developed by Horace
  
    print("%5s :%3d"%(k,v)) # 字串 : 數字
    names.append(k)
    values.append(v)

fig, axs = plt.subplots(1, 2, figsize=(18, 6), sharey=True)
axs[0].bar(names, values)
axs[1].plot(names, values)
fig.suptitle('直方圖 + 折線圖 by TA Grace')

````

[TOP](#000)


<a name="003"/>

### [5-3 Your Show Time: 找一篇簡體文章或新聞 實作H1~H6]


[TOP](#000)
